{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe457eab",
   "metadata": {},
   "source": [
    "# LightRAG application for Q&A on the Long AtR Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d61779",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, update_display\n",
    "\n",
    "import pdfplumber\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "from lightrag.utils import setup_logger, EmbeddingFunc\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "setup_logger(\"lightrag\", level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc3e65",
   "metadata": {},
   "source": [
    "## 1. Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095da7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DIR = \"./rag_data\"\n",
    "pdf_path = \"./AtR_guide.pdf\"\n",
    "\n",
    "if not os.path.exists(RAG_DIR):\n",
    "    os.mkdir(RAG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = \"\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        pdf_text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ded2e2",
   "metadata": {},
   "source": [
    "## 2. LightRAG initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6eccb7",
   "metadata": {},
   "source": [
    "### Use of OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_rag():\n",
    "    rag = LightRAG(\n",
    "        working_dir=RAG_DIR,\n",
    "        chunk_token_size=1200,\n",
    "        chunk_overlap_token_size=200,\n",
    "        llm_model_func=gpt_4o_mini_complete,\n",
    "        llm_model_name=\"gpt-4o-mini\",\n",
    "        llm_model_max_async=4,\n",
    "        llm_model_max_token_size=32768,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=3072,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: openai_embed(texts, model=\"text-embedding-3-large\")),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45424c3",
   "metadata": {},
   "source": [
    "### Use of Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag.llm.openai import openai_complete_if_cache\n",
    "\n",
    "\n",
    "async def groq_complete(\n",
    "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "async def initialize_rag():  # noqa\n",
    "    rag = LightRAG(\n",
    "        working_dir=RAG_DIR,\n",
    "        chunk_token_size=1200,\n",
    "        chunk_overlap_token_size=200,\n",
    "        llm_model_func=groq_complete,\n",
    "        llm_model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "        llm_model_max_async=4,\n",
    "        llm_model_max_token_size=32768,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=3072,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: openai_embed(texts, model=\"text-embedding-3-large\")),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffc7d4",
   "metadata": {},
   "source": [
    "## 3. Index Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70416e31",
   "metadata": {},
   "source": [
    "> **IMPORTANT NOTE:** DON'T run the following cell, unless you want to re-create the index, according to your desired configurations in the `initialize_rag` function. \n",
    "> \n",
    "> This index creation is the \"heart\" of LightRAG's functionality, since it is responsible for creating the vector and graph databases. In particular, after its execution, a bunch of json files are created in the directory defined by the `RAG_DIR` variable. D \n",
    ">\n",
    "> epending on the chosen models and/or providers chosen in step 2, the time of execution will vary. In particular, running this cell:\n",
    ">\n",
    "> 1. is (generally) time-consuming, since the provided pdf document is quite large.\n",
    ">\n",
    "> 2. can be costly depending on the chosen models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67467516",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_index():\n",
    "    \"\"\"\n",
    "    This function does all the hard-lift work.\n",
    "    It creates the index and returns the RAG instance.\n",
    "    \"\"\"\n",
    "    rag = None\n",
    "    try:\n",
    "        rag = await initialize_rag()\n",
    "        await rag.ainsert(pdf_text)\n",
    "        return rag\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if rag:\n",
    "            await rag.finalize_storages()\n",
    "    return rag\n",
    "\n",
    "\n",
    "rag_instance = await create_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300390f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5aa252",
   "metadata": {},
   "source": [
    "During inference, the most important parameters to configure can be divided into 2 categories:\n",
    "\n",
    "1. The ones that are related to the retrieval of the most relevant data. These parameters are crucial and the most important among them are:\n",
    "\n",
    "   * ***MODE*** (defaults to `hybrid`), which specifies how the most relevant data are retrieved.\n",
    "\n",
    "   * ***TOP_K*** (defaults to `60`), which specifies the number of top items to retrieve. \n",
    "\n",
    "2. The ones that are related to the processing of the retrieved data and the generation of the final response. Here the most important parameters are:\n",
    "are the following:\n",
    "\n",
    "   * ***USER_PROMPT*** (defaults to `None`), which specifies the prompt to be used during inference.\n",
    "\n",
    "   * ***MODEL_FUNC_OVERRIDE*** (defaults to the model used during index creation), which specifies the model to be used during inference. Therefore, it's an optional override for the LLM model function to use for this specific query.\n",
    "\n",
    "   * *[OPTIONAL]* For adapting the final response's layout, just configure the ***response_type*** parameter according to the docs of the `QueryParam` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following lines to write the response to a markdown file\n",
    "\n",
    "# QA_DIR = \"./QAs\"\n",
    "# filename = os.path.join(QA_DIR, \"sample_QAs.md\")\n",
    "\n",
    "# if not os.path.exists(QA_DIR):\n",
    "#     os.mkdir(QA_DIR)\n",
    "\n",
    "\n",
    "# def append_qa_to_markdown(question: str, answer: str, filename: str) -> None:\n",
    "#     \"\"\"Appends a question and the corrsponding answer to a markdown file.\"\"\"\n",
    "#     global QA_COUNTER\n",
    "\n",
    "#     with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "#         f.write(\n",
    "#             f\"# Question {QA_COUNTER}:\\n## {question}\\n\\n # *Answer:*\\n{answer}\\n\\n\"\n",
    "#         )\n",
    "\n",
    "#     QA_COUNTER += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf057135",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FUNC_OVERRIDE = None\n",
    "\n",
    "USER_PROMPT = \"\"\"You are a deeply realized individual, meaning you have been through all 7 stages, \\\n",
    "including the corresponding experiences and realizations described in the Awakening to Reality \\\n",
    "(AtR) guide, a book you wrote aimed for helping individuals discover their true selves. Having a \\\n",
    "background in Buddhism, you have a deep understanding of the relevant old texts of those spiritual \\\n",
    "tranditions as well. You are going to be asked questions from people who are interested in your \\\n",
    "proposed path to awakening, and who are either new or more experienced in your practices or \\\n",
    "further down the path as far as the depth and clarity of their realization is concerned. In your \\\n",
    "responses, please follow the instructions belows:\n",
    "    1. Answer only based on the information in the Awakening to Reality (AtR) guide.\n",
    "    2. Try your best to use the words and their associated meanings, as they are used in the guide.\n",
    "    3. Do not create any new information or fantasize about anything that is not in the guide.\n",
    "    4. If the question is not related to the Awakening to Reality (AtR) guide, do not answer, \\\n",
    "    explaining that you are only interested in questions relevant to the Awakening to Reality \\\n",
    "    (AtR) guide.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def display_lightrag_response(\n",
    "    question, user_prompt, mode_val, top_k_val, model_func_to_use, current_rag_instance\n",
    "):\n",
    "    \"\"\"Displays the response from the LighTRAG instance in a streaming manner in a Markdown cell.\"\"\"\n",
    "    try:\n",
    "        stream_iter = current_rag_instance.query(\n",
    "            question,\n",
    "            param=QueryParam(\n",
    "                mode=mode_val,\n",
    "                top_k=top_k_val,\n",
    "                model_func=model_func_to_use,\n",
    "                stream=True,\n",
    "                user_prompt=user_prompt,\n",
    "                response_type=\"Bullet Points\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display_id = \"lightrag_stream_output\"\n",
    "        full_response = \"\"\n",
    "\n",
    "        # Initial display (blank)\n",
    "        display(Markdown(\"\"), display_id=display_id)\n",
    "\n",
    "        # Append tokens/chunks as they come in\n",
    "        async for chunk in stream_iter:\n",
    "            full_response += chunk\n",
    "            update_display(Markdown(full_response), display_id=display_id)\n",
    "\n",
    "        return full_response\n",
    "\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"**Error during streaming:** `{type(e).__name__}: {e}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to clear the cache (if needed)\n",
    "await rag_instance.aclear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'hybrid'\n",
    "TOP_K = 60\n",
    "\n",
    "\n",
    "QUESTION = ...  # your question here\n",
    "\n",
    "response = await display_lightrag_response(\n",
    "    QUESTION, USER_PROMPT, MODE, TOP_K, MODEL_FUNC_OVERRIDE, rag_instance\n",
    ")\n",
    "\n",
    "\n",
    "## Uncomment the following lines to write the response to a markdown file\n",
    "\n",
    "# QA_COUNTER = 1  # counts the number of QA pairs written to markdown\n",
    "# append_qa_to_markdown(QUESTION, response, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
